# config/test_ppo_config.yaml
# This is a test configuration for PPO training with publicly available models
nim_reward:
  api_key: "${NVIDIA_NIM_API_KEY}"
  base_url: "https://integrate.api.nvidia.com/v1"
  model_id: "nvidia/llama-3.1-nemotron-70b-reward"
  max_retries: 3
  retry_delay: 2.0

embedding:
  model_id: "Lajavaness/bilingual-embedding-large"

data:
  dataset_name: "stanfordnlp/SHP"
  preprocessing:
    max_length: 512  # Reduced for faster processing
    batch_size: 4    # Smaller batch size for testing
    num_workers: 0
    cache_dir: "./cache"

model:
  input_dim: 2
  hidden_dims: [64, 32]
  output_dim: 1
  dropout: 0.1

training:
  seed: 42
  learning_rate: 0.0001
  weight_decay: 0.01
  num_epochs: 3      # Reduced for testing
  warmup_steps: 10
  gradient_accumulation_steps: 1
  evaluation_steps: 50
  save_steps: 100
  logging_steps: 10
  max_grad_norm: 1.0
  use_wandb: false   # Disable wandb for testing
  early_stopping_patience: 3

wandb:
  project: "realm-test-ppo"
  entity: "test-user"
  name: "test-ppo-run"

rlhf:
  ppo:
    model_name: "facebook/opt-125m"  # Using a publicly available model
    batch_size: 4
    mini_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 1e-5
    max_length: 256   # Reduced for testing
    kl_penalty: 0.2
    early_stopping: true
    target_kl: 0.1
    score_clip: 10.0
    num_epochs: 1
    max_steps: 100
    use_score_scaling: true
    use_score_norm: true
  
  dpo:
    model_name: "facebook/opt-125m"  # Using a publicly available model
    learning_rate: 5e-7
    batch_size: 2
    gradient_accumulation_steps: 1
    beta: 0.1  # Controls the KL penalty term in DPO loss function
    num_epochs: 3
    max_length: 512
    max_prompt_length: 256
    max_target_length: 256
    generation_batch_size: 4
    val_split: 0.05
    eval_steps: 500
    save_steps: 1000
    logging_steps: 100
    warmup_steps: 100
    weight_decay: 0.01
    max_grad_norm: 1.0
    use_peft: false  # Set to true to use parameter-efficient fine-tuning
    # PEFT/LoRA parameters (only used if use_peft is true)
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    save_merged_model: false  # Set to true to save the merged model (only for PEFT)
    # Optional quantization (for larger models)
    use_4bit: false
    use_8bit: false
