# config/config.yaml
nim_reward:
  api_key: "${NVIDIA_NIM_API_KEY}"
  base_url: "https://integrate.api.nvidia.com/v1"
  model_id: "nvidia/llama-3.1-nemotron-70b-reward"
  max_retries: 3
  retry_delay: 2.0

gemini:
  api_key: "${GEMINI_API_KEY}"
  model_id: "gemini-embedding-exp-03-07"

data:
  dataset_name: "stanfordnlp/SHP"
  preprocessing:
    max_length: 1024
    batch_size: 16
    num_workers: 0
    cache_dir: "./cache"

model:
  input_dim: 2
  hidden_dims: [64, 32]
  output_dim: 1
  dropout: 0.1

training:
  seed: 42
  learning_rate: 0.0001
  weight_decay: 0.01
  num_epochs: 10
  warmup_steps: 100
  gradient_accumulation_steps: 1
  evaluation_steps: 500
  save_steps: 1000
  logging_steps: 50
  max_grad_norm: 1.0
  use_wandb: true
  early_stopping_patience: 3

wandb:
  project: "llama-gemini-reward-model"
  entity: "dhairyagoswami2001"
  name: "shp-reward-model"

rlhf:
  ppo:
    model_name: "meta-llama/Meta-Llama-3.1-Instruct-8B"
    batch_size: 8
    mini_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 1.41e-5
    max_length: 512
    kl_penalty: 0.2
  
  dpo:
    model_name: "meta-llama/Meta-Llama-3.1-Instruct-8B"
    learning_rate: 5e-7
    batch_size: 4
    gradient_accumulation_steps: 1
    beta: 0.1
