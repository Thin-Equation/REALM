# config/config.yaml
llama_reward:
  model_id: "infly/INF-ORM-Llama3.1-70B"  # Changed to infly model
  quantization: "null"  # Options: "4bit", "8bit", or null for no quantization
  device_map: "auto"
  max_length: 2048

gemini:
  api_key: "${GEMINI_API_KEY}"
  model_id: "models/embedding-001"
  task_type: "retrieval_document"

data:
  dataset_name: "stanfordnlp/SHP"
  preprocessing:
    max_length: 1024
    batch_size: 16
    num_workers: 4
    cache_dir: "./cache"

model:
  input_dim: 2
  hidden_dims: [64, 32]
  output_dim: 1
  dropout: 0.1

training:
  seed: 42
  learning_rate: 1e-4
  weight_decay: 1e-5
  num_epochs: 10
  warmup_steps: 100
  gradient_accumulation_steps: 1
  evaluation_steps: 500
  save_steps: 1000
  logging_steps: 50
  max_grad_norm: 1.0
  use_wandb: true
  early_stopping_patience: 3

wandb:
  project: "llama-gemini-reward-model"
  entity: "your-entity"
  name: "shp-reward-model"

rlhf:
  ppo:
    model_name: "meta-llama/Meta-Llama-3.1-Instruct-8B"  # Base model to fine-tune
    batch_size: 8
    mini_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 1.41e-5
    max_length: 512
    kl_penalty: 0.2
  
  dpo:
    model_name: "meta-llama/Meta-Llama-3.1-Instruct-8B"  # Base model to fine-tune
    learning_rate: 5e-7
    batch_size: 4
    gradient_accumulation_steps: 1
    beta: 0.1  # KL penalty coefficient
